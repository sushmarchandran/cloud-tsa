# Get Started

Run the CloudTSA demos in your Kubernetes environment.

# Table of Contents
1. [Background](#background)
2. [Requirements](#requirements)
3. [Initialize](#initialization)
4. [Run the demos](#run)
    - [Demo 1: Predictively monitor gradual drifts in latency](#predict)
    - [Demo 2: Detect abrupt increases in latency](#abruptlatency)
    - [Demo 3: Detect abrupt increases in error](#abrupterror)
    - [Demo 4: Detect peaks in load](#peak)

<a name="background"></a>
## Background
These demos illustrate the various time series detection capabilities of CloudTSA
and how they can be combined with an application's performance metrics in Prometheus in order to generate real-time insights. The demos utilize a sample [Istio](https://istio.io) application.
This application consists of a collection of services which
are queried by a load generator. The behavior of these services are modified by the different demos. While Istio and Kubernetes are required for running the demos, CloudTSA can be used with any application whose metrics are available in a Prometheus database.

<a name="requirements"></a>
## Requirements

* Kubernetes >= 1.9. `kubectl` command needs to work.
* Istio >= 1.0.0. `istioctl` command needs to work.
  + Ensure your Istio is installed with Prometheus and Grafana addons enabled. Prometheus is enabled by default. To enable Grafana use the `grafana.enabled` flag in the Istio helm installer as follows.
    - `
    helm template install/kubernetes/helm/istio --name istio --set grafana.enabled=true --namespace istio-system > $HOME/istio.yaml
  `
* Download iter8 (`git clone https://github.com/istio-ecosystem/iter8.git`)
* Python >= 3.6
* Install python packages `requests`, `numpy` and `PyYAML` (`pip install requests numpy`; follow instructions on the [PyYAML](https://pyyaml.org/wiki/PyYAML) page.)
* Edit `iter8/cloudtsa/config/config.json` so that 'project_home' refers to absolute path of your CloudTSA project folder.
  - For example: `"project_home": "/home/istio/gitopen/iter8/cloudtsa",`

<a name="initialization"></a>
## Initialize
### Open two terminals
We will refer to them as Terminals 1 and 2.

### Port forward Grafana service
In Terminal 1, run the following command.
```
kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 &
```
**The above command** port forwards the Grafana service (installed as an Istio addon) to your localhost and local port *3000*. More information on using Istio and Grafana together is available
[here](https://istio.io/docs/tasks/telemetry/using-istio-dashboard/).

### Install and configure demo
In Terminal 1, navigate to the correct folder
and run the following command.
```
cd iter8/cloudtsa/demo
python democonfigure.py -dc democonfig.json
```

**The above command** deploys the sample application and the CloudTSA service in your Kubernetes cluster, extracts the web address at which HTTP queries can be sent to the sample application,
reconfigures Prometheus in your demo environment so that it can pull the insights generated by the CloudTSA service, and imports the CloudTSA dashboard into your Grafana service. To view the CloudTSA dashboard, go to your Grafana web page, click `Manage` under `Dashboards`, and click on `cloudtsa demo`.

#### A note on Grafana
> The CloudTSA demo dashboard contains one panel that shows the CloudTSA alerts and other panels that show application metrics. The layout of the panels in this documentation is different from the actual layout but you can navigate to the correct panels using the panel titles.
Further, there are three variables namely *entity*, *detector_type* and *metric_name* which are defined in the Grafana dashboard. Each demo requires you to select specific combinations of these variables.

### Start the load generator
In Terminal 2, navigate to the correct folder
and run the following command.
```
cd iter8/cloudtsa/demo
python ping.py -dc democonfig.json -t topology.json -p 20
```

**The above command** queries each service in the sample application at random at a total
frequency of ~20 queries / sec.

### Initialize the sample application and start the CloudTSA service
In Terminal 1, run the following command.
```
python demoinitialize.py -dc democonfig.json
```

**The above command** initializes the sample application by setting the mean processing delay of all its services to 0.1 sec and their failure rates (i.e., the probability of them aborting with a 500 HTTP status code) to 0.0. It also starts the CloudTSA service.

<a name="run"></a>
## Run the demos

<a name="predict"></a>
### Demo 1: Forecast latency and predict if latency will violate preset threshold
This demo illustrates how the predictive monitoring capabilities of CloudTSA can be combined with the latency metric of a service in order to detect significant increases in service latencies *before* they occur. **Start this demo by running**:
```
python demorun.py -dc democonfig.json -s gradual_latency
```

**In your CloudTSA dashboard in Grafana,** select `svc0` as *entity*, `predictivethresholds` and `thresholdpolicy` as *detector_type*, and `latency` as *metric_name*.  Then, navigate to the two graphs titled *Average Latency* and *CloudTSA Alerts*; you will see these graphs getting updated as in the following animation over a period of *8 min*.

<p align="center">
  <img src="https://raw.githubusercontent.com/istio-ecosystem/iter8-docs/master/cloudtsa/gif/gradual_latency.gif">
</p>

Service `svc0` experiences a gradual increase in mean latency from ~ 0.1 sec to ~ 8.0 sec which you can see on the graph on the left. CloudTSA learns this trend and creates two types of alerts for `svc0` which you can see on the graph on the right.

  1. A 'reactive' alert (yellow curve) *after* the mean latency value violates a preset threshold.

  2. A 'predictive' alert (blue curve) *before* the mean latency value violates a preset threshold. CloudTSA uses the Holt-Winters triple exponential smoothing algorithm to infer the increasing trend in latency and *predicts* that latency is likely to violate a preset threshold.

### Demo 2 (two parts): Detect abrupt increases in latency
This demo has two parts which illustrate how change detection capabilities of CloudTSA can be combined with the latency metric of a service in order to detect abrupt increases in service latencies. The change detection is based on the statistically robust CUSUM algorithm.

#### Part 1
**Start part 1 of this demo by running:**
```
python demorun.py -dc democonfig.json -s abrupt_latency_part1
```

**In your CloudTSA dashboard in Grafana,** select `svc1` as *entity*, `changedetection` and `thresholdpolicy` as *detector_type*, and `latency` as *metric_name*.  Then, continue looking at the two graphs titled to the two graphs titled *Average Latency* and *CloudTSA Alerts*; you will see these graphs getting updated as in the following animation over a period of *2 min*.

<p align="center">
  <img src="https://raw.githubusercontent.com/istio-ecosystem/iter8-docs/master/cloudtsa/gif/abrupt_latency_part1.gif">
</p>

The mean processing delay of `svc1` is abruptly increased from 0.1 sec to 5.0 sec as seen on the graph on the left. The change detection algorithm observes this abrupt change in behavior and fires an alarm as seen on the graph on the right.

#### Part 2
**Start this demo by running**:
```
python demorun.py -dc democonfig.json -s abrupt_latency_part2
```
**In your CloudTSA dashboard in Grafana,** you will see the graphs from part 1 change as follows over a period of *3 min*.

<p align="center">
  <img src="https://raw.githubusercontent.com/istio-ecosystem/iter8-docs/master/cloudtsa/gif/abrupt_latency_part2.gif">
</p>

In this part of the demo, the mean processing delay of `svc1` is increased evey further from 5.0 sec to 10.0 sec. The change detection algorithm observes this second abrupt change in the service latency and fires another alarm. The service latency has now breached a preset threshold which triggers a `thresholdpolicy` alarm as well.

<a name="abrupterror"></a>

### Demo 3: Detect abrupt increase in errors
This demo illustrates how the change detection capabilities of CloudTSA can be combined with the error count or error rate metric of a service in order to detect abrupt increases in errors. **Start this demo by running**:
```
python demorun.py -dc democonfig.json -s abrupt_errors
```
**In your CloudTSA dashboard in Grafana,** select `svc2` as *entity*, `changedetection` and `thresholdpolicy` as *detector_type*, and `error_count` as *metric_name*.  Then, navigate to the graphs titled *Error Count* and *CloudTSA Alerts*. The error rate related graphs are titled *Error Rate* and will require the selection of `error_rate` as a *metric_name*; you will see these graphs getting updated as in the following animation over a period of *3 min*.

<p align="center">
  <img src="https://raw.githubusercontent.com/istio-ecosystem/iter8-docs/master/cloudtsa/gif/abrupt_error_count.gif">
</p>

<p align="center">
  <img src="https://raw.githubusercontent.com/istio-ecosystem/iter8-docs/master/cloudtsa/gif/abrupt_error_rate.gif">
</p>

This demo shows the alerts that can be generated by CloudTSA when there is an abrupt increase in the errors generated by a service. The failure rate of the service `svc2` (i.e., the probability of the service aborting with a 500 HTTP status code) is abruptly increased from ~0.0 to ~0.2 . This increases both the error rate and error counts for `svc2`. The change detection algorithm observes both these abrupt changes in behavior and fires alarms. The increased error rates and counts also breach the preset thresholds of the `thresholdpolicy` detector which fires alarms as well.

<a name="peak"></a>
### Demo 4: Detect peaks in load
This demo illustrates how the peak detection capabilities of CloudTSA can be combined with the load metric of a service in order to detect peaks in load. **Start this demo by running**:
```
python demorun.py -dc democonfig.json -s peak
```

**The above command** creates fluctuations in the rate at which service `svc3`
is queried thereby creating peaks in its load metric.

**In your CloudTSA dashboard in Grafana,** select `svc3` as *entity*, `peakdetection` as *detector_type*, and `load` as *metric_name*.  Then, navigate to the two graphs titled *Total Load* and *CloudTSA Alerts*; you will see these graphs getting updated as in the following animation over a period of *5 min*.

<p align="center">
  <img src="https://raw.githubusercontent.com/istio-ecosystem/iter8-docs/master/cloudtsa/gif/peak.gif">
</p>

On the left, you can visualize the peaks in the load of service `svc3`. On the right, you see the corresponding alarms generated by CloudTSA for these peaks.
